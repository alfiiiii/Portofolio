{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> LECUN NETWORK\n"
      ],
      "metadata": {
        "id": "IbwJHzc1Lz9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "487tYyjtLvlO"
      },
      "outputs": [],
      "source": [
        "!pip install visualkeras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Resizing\n",
        "import visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "Af0TrcvxL3s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the input data\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "u2XF9SeEMGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand dimensions to include channel information and replicate grayscale images to 3 channels\n",
        "x_train = np.expand_dims(x_train, axis=3)\n",
        "x_test = np.expand_dims(x_test, axis=3)\n",
        "x_train = np.repeat(x_train, 3, axis=3)\n",
        "x_test = np.repeat(x_test, 3, axis=3)"
      ],
      "metadata": {
        "id": "SWLxYMxlMaVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "JvEKRTDqMmTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LeNet model\n",
        "lenet = Sequential(name=\"LeNet\")\n",
        "\n",
        "# Resize input to match 32x32 size used in original LeNet\n",
        "lenet.add(Resizing(32, 32, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Layer 1: Convolution + Average Pooling\n",
        "lenet.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 3)))\n",
        "lenet.add(AveragePooling2D(pool_size=(2, 2))) # Added pool_size=(2, 2)\n",
        "\n",
        "# Layer 2: Convolution + Average Pooling\n",
        "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
        "lenet.add(AveragePooling2D(pool_size=(2, 2))) # Added pool_size=(2, 2)\n",
        "\n",
        "# Flatten the output\n",
        "lenet.add(Flatten())\n",
        "\n",
        "# Fully connected layer 1\n",
        "lenet.add(Dense(120, activation='tanh'))\n",
        "\n",
        "# Fully connected layer 2\n",
        "lenet.add(Dense(84, activation='tanh'))\n",
        "\n",
        "# Output layer\n",
        "lenet.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the LeNet model\n",
        "lenet.compile(optimizer='adam', loss=\"mape\", metrics=['accuracy'])\n",
        "\n",
        "# Model summary:\n",
        "lenet.summary()"
      ],
      "metadata": {
        "id": "7dSxJB2FMy1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train = lenet.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = lenet.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "3VQQK6iTNBK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(lenet)"
      ],
      "metadata": {
        "id": "PHwpUsJhRXRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots()\n",
        "ax.plot([None] + train.history['accuracy']) # Now 'al' is defined and contains the training history\n",
        "ax.plot([None] + train.history['val_accuracy'])\n",
        "#Plot legend and use the best location automatically: loc = 0\n",
        "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
        "ax.set_title('Training/Validation per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('acc')"
      ],
      "metadata": {
        "id": "W3zgC7okRZ-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots()\n",
        "ax.plot([None] + train.history['loss'])\n",
        "ax.plot([None] + train.history['val_loss'])\n",
        "#Plot legend and use the best location automatically: loc = 0\n",
        "ax.legend(['Train loss', 'Validation loss'], loc = 0)\n",
        "ax.set_title('Training/Validation per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('loss')"
      ],
      "metadata": {
        "id": "UOwdEpReRgsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Alex Net"
      ],
      "metadata": {
        "id": "k-uZQRGyQPUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Resizing\n",
        "from tensorflow.keras.layers import BatchNormalization # Import BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import visualkeras\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2jQcgusBY53A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset using the Keras library:\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "eROSSJP_OENu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data using NumPy operations:\n",
        "x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)))\n",
        "x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)))\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "x_train = np.expand_dims(x_train, axis=3)\n",
        "x_test = np.expand_dims(x_test, axis=3)\n",
        "x_train = np.repeat(x_train, 3, axis=3)\n",
        "x_test = np.repeat(x_test, 3, axis=3)"
      ],
      "metadata": {
        "id": "96n97zWaQaEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train,10)  # converting labels into one-hot-encoded vectors\n",
        "y_test  = to_categorical(y_test,10)"
      ],
      "metadata": {
        "id": "X4NbXyzxQkm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and compile the AlexNet model:\n",
        "alexnet = Sequential(name=\"AlexNet\")\n",
        "\n",
        "alexnet.add(Resizing(227, 227, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Instead of accessing output_shape directly, infer it from the layer's configuration:\n",
        "output_shape = (None, 227, 227, 3)  # (batch_size, height, width, channels)\n",
        "print(\"Output shape:\", output_shape)\n",
        "#The Resizing layer's output shape can be inferred from height and width parameters provided.\n",
        "\n",
        "\n",
        "alexnet.add(Conv2D(filters=96, kernel_size=(11, 11),\n",
        "                   strides=(4, 4), activation=\"relu\",\n",
        "                   input_shape=(227, 227, 3)))\n",
        "\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(3, 3), strides= (2, 2)))\n",
        "alexnet.add(Conv2D(filters=256, kernel_size=(5, 5),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "alexnet.add(Conv2D(filters=384, kernel_size=(3, 3),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Conv2D(filters=384, kernel_size=(3, 3),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                        strides=(1, 1), activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "alexnet.add(Flatten())\n",
        "alexnet.add(Dense(4096, activation=\"relu\"))\n",
        "alexnet.add(Dropout(0.5))\n",
        "alexnet.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "alexnet.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary:\n",
        "alexnet.summary()"
      ],
      "metadata": {
        "id": "8ZQNF2-qQmpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model:\n",
        "al = alexnet.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=1)\n",
        "\n",
        "\n",
        "# Evaluate the model's performance on the test dataset:\n",
        "_, acc = alexnet.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiKZaBbVQt1a",
        "outputId": "675d9432-ca1c-48c3-a828-c4047736ba1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m353/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m37:37\u001b[0m 19s/step - accuracy: 0.7719 - loss: 6.4456"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(alexnet)"
      ],
      "metadata": {
        "id": "XHq9YXxJQ7Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots()ax.plot([None] + al.history['accuracy'])\n",
        "ax.plot([None] + al.history['val_accuracy'])\n",
        "#Plot legend and use the best location automatically: loc = 0\n",
        "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
        "ax.set_title('Training/Validation per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('acc')"
      ],
      "metadata": {
        "id": "C227M2eyREiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots()\n",
        "ax.plot([None] + al.history['loss'])\n",
        "ax.plot([None] + al.history['val_loss'])\n",
        "#Plot legend and use the best location automatically: loc = 0\n",
        "ax.legend(['Train loss', 'Validation loss'], loc = 0)\n",
        "ax.set_title('Training/Validation per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('loss')"
      ],
      "metadata": {
        "id": "fKjbHoKbRQ2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> VGGNET\n"
      ],
      "metadata": {
        "id": "_izf4h9uRG_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Resizing\n",
        "import visualkeras"
      ],
      "metadata": {
        "id": "17OTBcNfRQFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "doIG0yHPhGwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the input data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "metadata": {
        "id": "2kfb2nOohKmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand dimensions to include channel information and replicate grayscale images to 3 channels\n",
        "x_train = np.expand_dims(x_train, axis=3)\n",
        "x_test = np.expand_dims(x_test, axis=3)\n",
        "x_train = np.repeat(x_train, 3, axis=3)\n",
        "x_test = np.repeat(x_test, 3, axis=3)"
      ],
      "metadata": {
        "id": "jRzl4lcrhNHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "LkTrA5EqhO_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the VGGNet model (VGG-16-like architecture)\n",
        "vggnet = Sequential(name=\"VGGNet\")\n",
        "\n",
        "# Resize input to match 224x224 size used in VGGNet\n",
        "vggnet.add(Resizing(224, 224, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Block 1: Two Convolution layers + Max Pooling\n",
        "vggnet.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\n",
        "vggnet.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 2: Two Convolution layers + Max Pooling\n",
        "vggnet.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 3: Three Convolution layers + Max Pooling\n",
        "vggnet.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 4: Three Convolution layers + Max Pooling\n",
        "vggnet.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 5: Three Convolution layers + Max Pooling\n",
        "vggnet.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "vggnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Flatten the output\n",
        "vggnet.add(Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "vggnet.add(Dense(4096, activation='relu'))\n",
        "vggnet.add(Dense(4096, activation='relu'))\n",
        "\n",
        "# Output layer (Softmax layer)\n",
        "vggnet.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the VGGNet model\n",
        "vggnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "vggnet.summary()"
      ],
      "metadata": {
        "id": "t_ZH9FhUhRwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "vggnet = vggnet.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = vggnet.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "ARJKS2CBhWjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(vggnet)"
      ],
      "metadata": {
        "id": "8WDBt8S0rjh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots()\n",
        "ax.plot([None] + vggnet.history['accuracy']) # Now 'al' is defined and contains the training history\n",
        "ax.plot([None] + vggnet.history['val_accuracy'])\n",
        "#Plot legend and use the best location automatically: loc = 0\n",
        "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
        "ax.set_title('Training/Validation per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('acc')"
      ],
      "metadata": {
        "id": "rmH5psNLhd5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots()\n",
        "ax.plot([None] + vggnet.history['loss'])\n",
        "ax.plot([None] + vggnet.history['val_loss'])\n",
        "#Plot legend and use the best location automatically: loc = 0\n",
        "ax.legend(['Train loss', 'Validation loss'], loc = 0)\n",
        "ax.set_title('Training/Validation per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('loss')"
      ],
      "metadata": {
        "id": "DLgJuMUMhrKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_kdQtm6hwfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}