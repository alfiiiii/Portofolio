{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7e55850-862a-4004-8da4-66a0bd7681f7",
      "metadata": {
        "id": "d7e55850-862a-4004-8da4-66a0bd7681f7"
      },
      "source": [
        "###### Pinterest\n",
        "https://stackoverflow.com/questions/48225334/extracting-data-from-pinterest-using-beautifulsoup-python\n",
        "https://blog.apify.com/how-to-scrape-data-on-pinterest-in-5-easy-steps/\n",
        "https://github.com/iamatulsingh/pinterest-image-scrap\n",
        "\n",
        "###### web scrap\n",
        "https://github.com/alirezamika/autoscraper\n",
        "\n",
        "https://github.com/JustAnotherArchivist/snscrape\n",
        "\n",
        "https://github.com/instagram4j/instagram4j\n",
        "https://github.com/cinemagoer/cinemagoer/tree/master/docs\n",
        "https://github.com/joeyism/linkedin_scraper\n",
        "https://github.com/aliparlakci/bulk-downloader-for-reddit\n",
        "https://github.com/th3unkn0n/osi.ig\n",
        "https://github.com/kevinzg/facebook-scraper\n",
        "https://github.com/joeyism/linkedin_scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ab090182-1ade-4d09-a700-0437eaffd270",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "ab090182-1ade-4d09-a700-0437eaffd270",
        "outputId": "53c4ceb2-f479-4e45-a5aa-ed17e35cee35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinscrape\n",
            "  Downloading pinscrape-4.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pytest==7.2.0 (from pinscrape)\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests~=2.32.2 in /usr/local/lib/python3.10/dist-packages (from pinscrape) (2.32.3)\n",
            "Collecting beautifulsoup4==4.11.1 (from pinscrape)\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pydotmap~=0.1.3 (from pinscrape)\n",
            "  Downloading pydotmap-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opencv-python~=4.9.0.80 (from pinscrape)\n",
            "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy~=1.24.2 (from pinscrape)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.11.1->pinscrape) (2.6)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.0->pinscrape) (24.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.0->pinscrape) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.0->pinscrape) (24.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.0->pinscrape) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.0->pinscrape) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.0->pinscrape) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.2->pinscrape) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.2->pinscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.2->pinscrape) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.2->pinscrape) (2024.12.14)\n",
            "Downloading pinscrape-4.0.0-py3-none-any.whl (9.0 kB)\n",
            "Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydotmap-0.1.3-py3-none-any.whl (4.1 kB)\n",
            "Installing collected packages: pytest, pydotmap, numpy, beautifulsoup4, opencv-python, pinscrape\n",
            "\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beautifulsoup4-4.11.1 numpy-1.24.4 opencv-python-4.9.0.80 pinscrape-4.0.0 pydotmap-0.1.3 pytest-7.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "1aaab8ac712d4a9994ad458231671464"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --user pinscrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8c00c6e7-2be0-47d3-a414-02730d981345",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c00c6e7-2be0-47d3-a414-02730d981345",
        "outputId": "03690739-3d75-4e27-dc30-2e377afe65ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3799062a-9767-42b3-bd22-342191f4059f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3799062a-9767-42b3-bd22-342191f4059f",
        "outputId": "a08d3a6b-35cd-42ab-f1ed-ad221c414365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Using cached pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "790cc0b0-eb7b-4269-b6e7-f0d29dc0d68e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "790cc0b0-eb7b-4269-b6e7-f0d29dc0d68e",
        "outputId": "6fdd1bd7-3a06-45c9-a335-cb03418c6d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Using cached Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.10/dist-packages (4.10.1)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Using cached twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Using cached cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Using cached itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Using cached parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Using cached queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Using cached service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Using cached w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Using cached zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Collecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.10.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "\u001b[33mWARNING: pymongo 4.10.1 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo[srv]) (2.7.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (24.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.16.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.12.14)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.10.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.10.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.9.1 protego-0.3.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.2.1 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy pymongo[srv]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4204f932-2a36-4077-8728-e128c1f71e33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4204f932-2a36-4077-8728-e128c1f71e33",
        "outputId": "3da66887-1ff0-47e5-bfb5-46a986027d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /root/.local/lib/python3.10/site-packages (4.11.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient # Import the MongoClient class\n",
        "!ping ac-deevsdi-shard-00-00.0wod46s.mongodb.net\n",
        "\n",
        "democlient = MongoClient()\n",
        "# Import the pymongo module here\n",
        "import pymongo\n",
        "myclient = pymongo.MongoClient('mongodb://pjbl_gambar:pinterest@ac-deevsdi-shard-00-00.0wod46s.mongodb.net:27017,ac-deevsdi-shard-00-01.0wod46s.mongodb.net:27017,ac-deevsdi-shard-00-02.0wod46s.mongodb.net:27017/?replicaSet=atlas-143ed3-shard-0&ssl=true&authSource=admin')\n",
        "print (myclient.list_database_names())"
      ],
      "metadata": {
        "id": "1AQ1vfJM-quQ"
      },
      "id": "1AQ1vfJM-quQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_scraper = PinterestImageScraper()\n",
        "is_downloaded = p_scraper.make_ready()\n",
        "if is_downloaded:\n",
        "    print(\"\\nDownloading completed !!\")\n",
        "else:\n",
        "    print(\"\\nNothing to download !!\")"
      ],
      "metadata": {
        "id": "WldN9wyK-tKC"
      },
      "id": "WldN9wyK-tKC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape Pinterest images based on keyword search\n",
        "def scrape_pinterest_images(keyword):\n",
        "    url = f'https://www.pinterest.com/search/pins/?q={keyword}'\n",
        "    response = requests.get(url)\n",
        "    soup_scrape = soup(response.text, 'html.parser')\n",
        "    image_urls = []\n",
        "\n",
        "    for img in soup_scrape.find_all('img', {'src': True}):\n",
        "        img_url = img['src']\n",
        "        if '236x' in img_url:\n",
        "            img_url = img_url.replace('236x', '736x')\n",
        "            image_urls.append(img_url)\n",
        "\n",
        "    return image_urls\n",
        "\n",
        "# Keyword for Pinterest search\n",
        "keyword = 'cats'\n",
        "\n",
        "# Scrape Pinterest images based on keyword\n",
        "image_urls = scrape_pinterest_images(keyword)\n",
        "\n",
        "# Function to process image data\n",
        "def process_image(url):\n",
        "    response = requests.get(url)\n",
        "    image_data = response.content\n",
        "    return {'image': image_data}\n",
        "\n",
        "# Prepare a list of image data to insert\n",
        "image_data_list = [process_image(url) for url in image_urls]"
      ],
      "metadata": {
        "id": "IqRycpnJ-vdE"
      },
      "id": "IqRycpnJ-vdE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "from requests import get\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pydotmap import DotMap\n",
        "\n",
        "# Connect to MongoDB\n",
        "myclient= pymongo.MongoClient('mongodb://pjbl_gambar:pinterest@ac-deevsdi-shard-00-00.0wod46s.mongodb.net:27017,ac-deevsdi-shard-00-01.0wod46s.mongodb.net:27017,ac-deevsdi-shard-00-02.0wod46s.mongodb.net:27017/?replicaSet=atlas-143ed3-shard-0&ssl=true&authSource=admin')\n",
        "db = myclient['dataset']\n",
        "collection = db['images_avatar']\n",
        "\n",
        "class PinterestImageScraper:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.json_data_list = []\n",
        "        self.unique_img = []\n",
        "\n",
        "    @staticmethod\n",
        "    def clear():\n",
        "        if os.name == 'nt':\n",
        "            _ = os.system('cls')\n",
        "        else:\n",
        "            _ = os.system('clear')\n",
        "\n",
        "    # ---------------------------------------- GET GOOGLE RESULTS ---------------------------------\n",
        "    @staticmethod\n",
        "    def get_pinterest_links(body):\n",
        "        searched_urls = []\n",
        "        html = soup(body, 'html.parser')\n",
        "        links = html.select('#main > div > div > div > a')\n",
        "        print('[+] saving results ...')\n",
        "        for link in links:\n",
        "            link = link.get('href')\n",
        "            link = re.sub(r'/url\\?q=', '', link)\n",
        "            if link[0] != \"/\" and \"pinterest\" in link:\n",
        "                searched_urls.append(link)\n",
        "\n",
        "        return searched_urls\n",
        "\n",
        "    # -------------------------- save json data from source code of given pinterest url -------------\n",
        "    def get_source(self, url):\n",
        "        try:\n",
        "            res = get(url)\n",
        "        except Exception as e:\n",
        "            return\n",
        "        html = soup(res.text, 'html.parser')\n",
        "        # get json data from script tag having id initial-state\n",
        "        json_data = html.find_all(\"script\", attrs={\"id\": \"__PWS_DATA__\"})\n",
        "        for a in json_data:\n",
        "            self.json_data_list.append(a.string)\n",
        "\n",
        "    # --------------------------- READ JSON OF PINTEREST WEBSITE ----------------------\n",
        "    def save_image_url(self):\n",
        "        print('[+] saving image urls ...')\n",
        "        url_list = [i for i in self.json_data_list if i.strip()]\n",
        "        if not len(url_list):\n",
        "            return url_list\n",
        "        url_list = []\n",
        "        for js in self.json_data_list:\n",
        "            try:\n",
        "                data = DotMap(json.loads(js))\n",
        "                urls = []\n",
        "                for pin in data.props.initialReduxState.pins:\n",
        "                    if isinstance(data.props.initialReduxState.pins[pin].images.get(\"orig\"), list):\n",
        "                        for i in data.props.initialReduxState.pins[pin].images.get(\"orig\"):\n",
        "                            urls.append(i.get(\"url\"))\n",
        "                    else:\n",
        "                        urls.append(data.props.initialReduxState.pins[pin].images.get(\"orig\").get(\"url\"))\n",
        "\n",
        "                for url in urls:\n",
        "                    url_list.append(url)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        return list(set(url_list))\n",
        "\n",
        "    # ------------------------------ image hash calculation -------------------------\n",
        "    def dhash(self, image, hashSize=8):\n",
        "        resized = cv2.resize(image, (hashSize + 1, hashSize))\n",
        "        diff = resized[:, 1:] > resized[:, :-1]\n",
        "        return sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v])\n",
        "\n",
        "    # ------------------------------  save all downloaded images to folder ---------------------------\n",
        "    def saving_op(self, var):\n",
        "        url_list, folder_name = var\n",
        "        if not os.path.exists(os.path.join(os.getcwd(), folder_name)):\n",
        "                os.mkdir(os.path.join(os.getcwd(), folder_name))\n",
        "        for img in tqdm(url_list):\n",
        "            result = get(img, stream=True).content\n",
        "            file_name = img.split(\"/\")[-1]\n",
        "            file_path = os.path.join(os.getcwd(), folder_name, file_name)\n",
        "            img_arr = np.asarray(bytearray(result), dtype=\"uint8\")\n",
        "            image = cv2.imdecode(img_arr, cv2.IMREAD_COLOR)\n",
        "            if not self.dhash(image) in self.unique_img:\n",
        "                cv2.imwrite(file_path, image)\n",
        "            self.unique_img.append(self.dhash(image))\n",
        "            print(\"\", end=\"\\r\")\n",
        "\n",
        "    # ------------------------------  download images from image url list ----------------------------\n",
        "    def download(self, url_list, keyword):\n",
        "        folder_name = keyword\n",
        "        num_of_workers = 15\n",
        "        idx = len(url_list) // num_of_workers if len(url_list) > 9 else len(url_list)\n",
        "        param = []\n",
        "        for i in range(num_of_workers):\n",
        "            param.append((url_list[((i*idx)):(idx*(i+1))], folder_name))\n",
        "        with ThreadPoolExecutor(max_workers=num_of_workers) as executor:\n",
        "            executor.map(self.saving_op, param)\n",
        "        PinterestImageScraper.clear()\n",
        "\n",
        "    # -------------------------- get user keyword and google search for that keywords ---------------------\n",
        "    @staticmethod\n",
        "    def start_scraping(key=None):\n",
        "        try:\n",
        "            key = input(\"Enter keyword: \") if key == None else key\n",
        "            keyword = key + \" pinterest\"\n",
        "            keyword = keyword.replace(\"+\", \"%20\")\n",
        "            url = f'http://www.google.co.in/search?hl=en&q={keyword}'\n",
        "            print('[+] starting search ...')\n",
        "            res = get(url)\n",
        "            searched_urls = PinterestImageScraper.get_pinterest_links(res.content)\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "        return searched_urls, key.replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "    def make_ready(self, key=None):\n",
        "        extracted_urls, keyword = PinterestImageScraper.start_scraping(key)\n",
        "\n",
        "        print('[+] saving json data ...')\n",
        "        for i in extracted_urls:\n",
        "            self.get_source(i)\n",
        "\n",
        "        # get all urls of images and save in a list\n",
        "        #process_image = self.get_source()\n",
        "        url_list = self.save_image_url()\n",
        "        image_data_list = [process_image(i) for i in url_list]\n",
        "        collection.insert_many(image_data_list)\n",
        "        print('done save to mongocompass')\n",
        "\n",
        "        # download images from saved images url\n",
        "        print(f\"[+] Total {len(url_list)} files available to download.\")\n",
        "        print()\n",
        "\n",
        "        if len(url_list):\n",
        "            try:\n",
        "                self.download(url_list, keyword)\n",
        "            except KeyboardInterrupt:\n",
        "                return False\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    #if __name__ == \"__main__\":\n",
        "p_scraper = PinterestImageScraper()\n",
        "is_downloaded = p_scraper.make_ready()\n",
        "if is_downloaded:\n",
        "    print(\"\\nDownloading completed !!\")\n",
        "else:\n",
        "    print(\"\\nNothing to download !!\")"
      ],
      "metadata": {
        "id": "umFXva3ghzlL"
      },
      "id": "umFXva3ghzlL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mongodb://pjbl_gambar:<password>@ac-deevsdi-shard-00-00.0wod46s.mongodb.net:27017,ac-deevsdi-shard-00-01.0wod46s.mongodb.net:27017,ac-deevsdi-shard-00-02.0wod46s.mongodb.net:27017/?replicaSet=atlas-143ed3-shard-0&ssl=true&authSource=admin"
      ],
      "metadata": {
        "id": "FFfrYu5t-19-"
      },
      "id": "FFfrYu5t-19-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}